<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names: </div>

		<br>

		Link to webpage:  <a href="https://cal-cs184-student.github.io/hw-webpages-alper-test/hw3/index.html">https://cal-cs184-student.github.io/hw-webpages-alper-test/hw3/index.html</a>
		Link to GitHub repository:  <a href="https://github.com/cal-cs184-student/sp25-hw3-alper-time">https://github.com/cal-cs184-student/sp25-hw3-alper-time</a>
		


		<h2>Overview</h2>
			In this project we started from fundamental ray generation and led up to adaptive sampling techniques. We were able to get the full spectrum of commonly used ray-tracing illumination methodologies in modern computer graphics, and also have the flexibility to explore how to improve them as much as possible. This project took me an extremely long time, and had to restart 2 seperate times due to something breaking in the render engine that TA's were unable to help fix. There are things with the direct illumination that have some inefficiencies and unideal behavior, but the final accumulated result is near to what is expected. Overall, very interesting but challenging project.
		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		
		<h3>Ray Generation</h3>
		<p>In the ray generation process, I implemented a camera model that creates rays originating from a camera position and passing through each pixel of our image plane. For each pixel (x,y) in the image:</p>
		<ol>
			<li>Computed normalized device coordinates (NDC) by transforming pixel coordinates to the [-1,1] range</li>
			<li>Then it calculated the ray's direction vector by transforming these NDC coordinates based on the camera's field of view, position, and orientation</li>
			<li>Each ray is normalized and cast into the scene from the camera's position</li>
		</ol>
		
		<h3>Scene Intersection</h3>
		<p>After generating rays, I implemented intersection algorithms for various primitives in the scene:</p>
		
		<h4>Triangle Intersection</h4>
		<p>For triangle intersection, I implemented the Möller–Trumbore algorithm. This method works by:</p>
		<ol>
			<li>Representing a point on the triangle as a barycentric combination of its vertices: <code>P = (1-b1-b2)V0 + b1V1 + b2V2</code> where b1 and b2 are barycentric coordinates</li>
			<li>Setting up an equation for the ray intersection: <code>O + tD = (1-b1-b2)V0 + b1V1 + b2V2</code> where O is the ray origin, D is the ray direction, and t is the distance along the ray</li>
			<li>Solving this system of equations for t, b1, and b2 it checks:</li>
			<li>If the intersection is valid by ensuring:
				<ul>
					<li>t > 0 (intersection is in front of the ray)</li>
					<li>0 ≤ b1 ≤ 1 and 0 ≤ b2 ≤ 1 and b1 + b2 ≤ 1 (intersection point is inside the triangle)</li>
				</ul>
			</li>
		</ol>
		<p>If all conditions are met, it then computes the intersection point, normal, and record other necessary information for shading calculations.</p>
		
		<h4>Sphere and Other Primitive Intersections</h4>
		<p>For spheres, I implemented the quadratic equation approach by:
			<ol>
				<li>Computing coefficients of the quadratic equation based on the ray and sphere parameters</li>
				<li>Calculating the discriminant to determine if an intersection exists</li>
				<li>Finding the closest valid intersection point (if any)</li>
			</ol>
		</p>
		
	
		
		<div style="display: flex; justify-content: space-between; margin-bottom: 10px;">
			<figure style="width: 32%;">
				<img src="images/part1/CBbunny.png" alt="Bunny in Cornell box" style="width:100%"/>
				<figcaption>Bunny in Cornell box</figcaption>
			</figure>
			<figure style="width: 32%;">
				<img src="images/part1/CBgems.png" alt="Gems in Cornell box" style="width:100%"/>
				<figcaption>Gems in Cornell box</figcaption>
			</figure>
			<figure style="width: 32%;">
				<img src="images/part1/CBspheres.png" alt="Spheres in Cornell box" style="width:100%"/>
				<figcaption>Spheres in Cornell box</figcaption>
			</figure>
		</div>

		<h2>Part 2: Bounding Volume Hierarchy</h2>
		
		<h3>BVH Construction Algorithm</h3>
		<p>My BVH implementation uses a recursive top-down construction approach that divides primitives into spatially coherent groups. Here's the step-by-step process:</p>
		
		<ol>
			<li><strong>Bounding Box Computation:</strong> For each node, a bbox is computed that encloses all the primitives in the BVH leaf node.</li>
			<li><strong>Termination Check:</strong> If the number of primitives is less than or equal to the maximum leaf size (4 in our implementation), a leaf node is created</li>
			<li><strong>Axis Selection:</strong> Otherwise, the splitting axis is found by finding the dimension with the largest extent in the bounding box</li>
			<li><strong>Centroid-based Sorting:</strong> Then, sort all primitives along this axis based on their bounding box centroids</li>
			<li><strong>Median Split:</strong> Split the primitives at the median point (exactly half the primitives go to each child)</li>
			<li><strong>Recursion:</strong> Recursively build the left and right subtrees</li>
		</ol>
		
		<h3>Splitting Heuristic</h3>
		<p>For the splitting heuristic, I implemented a simple median-based approach. This heuristic was chosen for its balance of effectiveness and efficiency, and was suggested:</p>
		
		<ul>
			<li>Identify the axis with the largest extent in the node's bounding box, as this dimension offers the greatest opportunity for effective spatial division</li>
			<li>Sort all primitives along this axis based on their bounding box centroids</li>
			<li>The split point is placed at the median, dividing the primitives into two equal-sized groups</li>
		</ul>
		
	
		
		<h3>Performance Analysis</h3>
		<p>Conducted an analysis of the two BVH implementations by comparing rendering times with and without the acceleration structure:</p>
		
		<ul>
			<li>Without BVH: Each ray must test against every primitive in the scene (O(n) complexity)</li>
			<li>With BVH: Each ray traverses the tree with O(log n) complexity, only testing against primitives in relevant leaf nodes</li>
		</ul>
		
		<p>The below shows the comparison in rendering times for fairly complex scenes.</p>
		
		<div style="display: flex; flex-direction: column; gap: 20px; margin-bottom: 20px;">
			<div style="display: flex; justify-content: space-between; margin-bottom: 10px;">
				<figure style="width: 48%;">
					<img src="images/part2/cow_slow.png" alt="Cow without Accel BVH" style="width:100%"/>
					<figcaption>Cow without Accel BVH</figcaption>
				</figure>
				<figure style="width: 48%;">
					<img src="images/part2/cow_fast.png" alt="Cow with Accel BVH" style="width:100%"/>
					<figcaption>Cow with Accel BVH</figcaption>
				</figure>
			</div>
			<div style="display: flex; justify-content: space-between; margin-bottom: 10px;">
				<figure style="width: 48%;">
					<img src="images/part2/max_slow.png" alt="Max without Accel BVH" style="width:100%"/>
					<figcaption>Max without Accel BVH</figcaption>
				</figure>
				<figure style="width: 48%;">
					<img src="images/part2/max_fast.png" alt="Max with Accel BVH" style="width:100%"/>
					<figcaption>Max with Accel BVH</figcaption>
				</figure>
			</div>
			<div style="display: flex; justify-content: space-between; margin-bottom: 10px;">
				<figure style="width: 48%;">
					<img src="images/part2/lucy_slow.png" alt="Lucy without Accel BVH" style="width:100%"/>
					<figcaption>Lucy without Accel BVH</figcaption>
				</figure>
				<figure style="width: 48%;">
					<img src="images/part2/lucy_fast.png" alt="Lucy with Accel BVH" style="width:100%"/>
					<figcaption>Lucy with Accel BVH</figcaption>
				</figure>
			</div>
		</div>

		<h2>Part 3: Direct Illumination</h2>
		<h3>Hemisphere Sampling</h3>
		<p>In this strategy, random directions are sampled over the hemisphere, thus a constant PDF of 1/2PI is used for uniform sampling. For every sample:</p>
		<ol>
			<li>Generate a random direction in the hemisphere</li>
			<li>Cast a ray in that direction WITHIN the hemisphere</li>
			<li>If the ray hits a light source, evaluate the BSDF at the hit point, calculate the contribution of the hit, and add to the total radiance</li>
		</ol>
		After each sample is evvaluated, the rolling sum is averaged which is the output. This process is highly inefficient as most random rays never end up hitting light sources, which wastes valuable compute time on non-contributing samples.

		<h3>Importance Sampling</h3>
		<p>In this strategy, directions are sampled directly from light sources, so samples are guaranteed to hit. For each light in the scene:</p>
		<ol>
			<li>Get a sample point on the light</li>
			<li>Skip if the PDF is zero or no radiance is output or if the light is below the hemisphere</li>
			<li>Cast a shadow ray to check visisbility</li>
			<li>If the ray is visible, evaluate the BSDF, cakculate the contribution based off the montecarlo estimator formulation, and add to the total radiance</li>
		</ol>
		After each sample is evaluated, average the total radiance by the number of samples per light. 
		<h3>Comparison of Hemisphere vs. Importance Sampling</h3>
		<p>Below are comparisons of the two sampling strategies on different scenes. Notice how importance sampling produces cleaner results with the same number of samples, as it focuses computation on paths that contribute to the final image. Rendered all with -s set to 16 and -l set to 8 in order to keep rendering time reasonable.</p>
		
		<div style="display: flex; justify-content: space-between; margin-bottom: 10px;">
			<figure style="width: 48%;">
				<img src="images/part3/CBbunny_H_16_8.png" alt="Bunny with Hemisphere Sampling" style="width:100%"/>
				<figcaption>Bunny with Hemisphere Sampling</figcaption>
			</figure>
			<figure style="width: 48%;">
				<img src="images/part3/CBbunny_16_8.png" alt="Bunny with Importance Sampling" style="width:100%"/>
				<figcaption>Bunny with Importance Sampling</figcaption>
			</figure>
		</div>
		<div style="display: flex; justify-content: space-between; margin-bottom: 10px;">
			<figure style="width: 48%;">
				<img src="images/part3/CB_spheres_H.png" alt="Spheres with Hemisphere Sampling" style="width:100%"/>
				<figcaption> Spheres with Hemisphere Sampling</figcaption>
			</figure>
			<figure style="width: 48%;">
				<img src="images/part3/CB_spheres.png" alt="Cornell Box Spheres with Importance Sampling" style="width:100%"/>
				<figcaption>Cornell Box Spheres with Importance Sampling</figcaption>
			</figure>
		</div>
		Overall, importance sampling produces much less noisy results with the same sampling settings, and with considerably less computational time. This efficiency comes from the fact that importance sampling focuses computational resources on paths that are more likely to contribute to the final image. By directly sampling from light sources, we ensure that each sample has the potential to contribute illumination, unlike hemisphere sampling where many rays may never intersect with any light source.
		
		In my implementation, the performance difference was substantial. For the Cornell Box scenes, importance sampling rendered in approximately half the time compared to hemisphere sampling while producing cleaner images with better-defined shadows and less grain. This difference becomes even more pronounced in scenes with small or distant light sources, where the probability of randomly hitting a light with hemisphere sampling becomes extremely low.
		
		The mathematical advantage of importance sampling is that it reduces variance in the Monte Carlo estimator by sampling according to a distribution that's proportional to the integrand. In the context of direct illumination, this means sampling proportionally to the light's contribution, which leads to faster convergence and more efficient use of computational resources. Although hemisphere sampling is much easier to implement, it is very computationally inefficient.
		<h3>Light Ray Sampling Rate Comparison</h3>
		
		<div style="display: grid; grid-template-columns: 1fr 1fr; grid-gap: 20px; margin-bottom: 20px;">
			<figure style="margin: 0;">
				<img src="images/part3/dragon_1_light.png" alt="Dragon with 1 Light Ray" style="width:100%"/>
				<figcaption>Dragon with 1 Light Ray per Sample</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part3/dragon_4_light.png" alt="Dragon with 4 Light Rays" style="width:100%"/>
				<figcaption>Dragon with 4 Light Rays per Sample</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part3/dragon_16_light.png" alt="Dragon with 16 Light Rays" style="width:100%"/>
				<figcaption>Dragon with 16 Light Rays per Sample</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part3/dragon_64_light.png" alt="Dragon with 64 Light Rays" style="width:100%"/>
				<figcaption>Dragon with 64 Light Rays per Sample</figcaption>
			</figure>
		</div>
		
		<p>As shown in the images above, increasing the number of light rays significantly reduces noise in the shadows and improves the overall image quality. With just 1 ray per sample, the image shows considerable noise, especially in shadow regions. At 64 rays per sample, the image achieves a much smoother appearance with well-defined shadows, though at a substantially higher computational cost.</p>
		<h2>Part 4: Global Illumination</h2>
		<h3>Indirect lighting function explanation:</h3>
		<p>
			The indirect lighting implementation uses path tracing w/ Monte Carlo integration and russian roulette early termination mechanism to efficiently simulate global illumination via light bouncing through the scene. First, a local coordinate system is established at the hit point, then the one-bounce lighting is accumulated into the output radiance. The function checks termination conditions, terminating if the max ray depth has been reached. To compute indirect lighting, the function samples a new Ray
			direction from the BSDF of the surface, which basically determines how light scatters based on the material properties. This sampling provides both a vector direction AND a PDF for that specific direction. If either the PDF or the BSDF for the direction are zero, the func skips it. The sampled direction is transformed to world space coords, and a new ray is created starting infintesimally small offset above the hit point in order to avoid self-intersection. The ray's depth is incremented, to preserve the recursive
			termination structure, and then russian roulette early termination is applied with a probability based off the reflectivity of the surface. This method non-deterministically terminates paths while maintaining an unbiased estimate property. If the path continues, the new ray is traced through the scene. Once the ray has a new hit point, the function does a recursive call to compute radiance from that point. The recursive call captures radiance from ALL subsequent bounces. The recursive radiance is then weighted by the BSDF eval for the ray's direction, the cosine term, and then divided by the PDF and the russian roulette continuation probability. 
			The final radiance combines both direct AND indirect lighting components, creating a global illumination solution that captures complex lighting effects as written in the project spec.
		</p>
		<h3>Global Illumination Results</h3>
		<p>
			Below are images rendered with global illumination at 1024 samples per pixel. 
		</p>
		
		<div style="display: flex; justify-content: space-between; margin-bottom: 20px;">
			<figure style="width: 48%;">
				<img src="images/part4/spheres_1024_global.png" alt="Spheres with Global Illumination" style="width:100%"/>
				<figcaption>Spheres with Global Illumination (1024 samples)</figcaption>
			</figure>
			<figure style="width: 48%;">
				<img src="images/part4/blob_1024_global.png" alt="Blob with Global Illumination" style="width:100%"/>
				<figcaption>Blob with Global Illumination (1024 samples)</figcaption>
			</figure>
		</div>
		
		<h3>Direct vs Indirect Illumination Comparison</h3>
		
		
		<div style="display: grid; grid-template-columns: 1fr 1fr; grid-gap: 20px; margin-bottom: 20px;">
			<figure style="margin: 0;">
				<img src="images/part4/blob_1024_direct.png" alt="Only Direct Illumination" style="width:100%"/>
				<figcaption>Only Direct Illumination (One Bounce)</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part4/blob_1024_global.png" alt="Only Indirect Illumination" style="width:100%"/>
				<figcaption>Only Indirect Illumination (Multiple Bounces)</figcaption>
			</figure>
		</div>
		
		<p>
			The direct illumination image (left) shows only the light that comes directly from light sources to surfaces. This creates strong highlights and sharp shadows but misses the subtle color bleeding and soft illumination in shadowed areas.
		</p>
		
		<p>
			The indirect illumination image (right) shows only the light that has bounced at least once off other surfaces. This component is responsible for color bleeding (where colors from one surface affect nearby surfaces), ambient occlusion effects, and the soft illumination in areas not directly visible to light sources.
		</p>

		<h3> Ray Depth Comparison Without Accumulation</h3>
		<p>GRADER NOTE: I attempted to debug this issue for approximately 6 hours, but was unable to fix it. When I turned off the accumulation, I got results very different to what shouldve been the output. For that reason, for the comparison I ran renders at 1024 samples with the default "-o" setting, which led to the images you see below.</p>
		<div style="display: grid; grid-template-columns: 1fr 1fr 1fr; grid-gap: 15px; margin-bottom: 20px;">
			<figure style="margin: 0;">
				<img src="images/part4/CBbunny_m0_v2.png" alt="Max Ray Depth: 0" style="width:100%"/>
				<figcaption>Max Ray Depth: 0</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part4/CBbunny_m1_v2.png"" alt="Max Ray Depth: 1" style="width:100%"/>
				<figcaption>Max Ray Depth: 1</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part4/CBbunny_m2_v2.png"" alt="Max Ray Depth: 2" style="width:100%"/>
				<figcaption>Max Ray Depth: 2</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part4/CBbunny_m3_v2.png"" alt="Max Ray Depth: 3" style="width:100%"/>
				<figcaption>Max Ray Depth: 3</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part4/CBbunny_m4_v2.png"" alt="Max Ray Depth: 4" style="width:100%"/>
				<figcaption>Max Ray Depth: 4</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part4/CBbunny_m5_v2.png"" alt="Max Ray Depth: 100" style="width:100%"/>
				<figcaption>Max Ray Depth: 5</figcaption>
			</figure>
		</div>
		<h3>Ray Depth Comparison with Russian Roulette</h3>
		
		<p>
			The following images demonstrate the effect of different maximum ray depth settings when using Russian Roulette for path termination. The max_ray_depth parameter (-m flag) controls how many bounces of light are simulated before Russian Roulette takes over as the sole termination mechanism.
		</p>
		
		<div style="display: grid; grid-template-columns: 1fr 1fr 1fr; grid-gap: 15px; margin-bottom: 20px;">
			<figure style="margin: 0;">
				<img src="images/part4/rr/CBbunny_m0_rr.png" alt="Max Ray Depth: 0" style="width:100%"/>
				<figcaption>Max Ray Depth: 0</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part4/rr/CBbunny_m1_rr.png" alt="Max Ray Depth: 1" style="width:100%"/>
				<figcaption>Max Ray Depth: 1</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part4/rr/CBbunny_m2_rr.png" alt="Max Ray Depth: 2" style="width:100%"/>
				<figcaption>Max Ray Depth: 2</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part4/rr/CBbunny_m3_rr.png" alt="Max Ray Depth: 3" style="width:100%"/>
				<figcaption>Max Ray Depth: 3</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part4/rr/CBbunny_m4_rr.png" alt="Max Ray Depth: 4" style="width:100%"/>
				<figcaption>Max Ray Depth: 4</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part4/rr/CBbunny_m100.png" alt="Max Ray Depth: 100" style="width:100%"/>
				<figcaption>Max Ray Depth: 100</figcaption>
			</figure>
		</div>
		
		
		<ul>
			<li>At depth 0, only direct lighting is visible with no indirect bounces</li>
			<li>At depth 1, we see one bounce of indirect light, adding some color bleeding</li>
			<li>At depths 2-4, each additional bounce adds more subtle lighting effects</li>
			<li>At depth 100, Russian Roulette naturally terminates most paths after a few bounces, but allows for rare long paths that can capture subtle lighting effects</li>
		</ul>
		
		<p>
			Russian Roulette ensures that even with a high maximum depth, the renderer remains efficient by probabilistically terminating paths while maintaining an unbiased result. Compared to the results in the previous section, there is marginally more noise, but the compute time is significantly less.
		</p>

		
		<h3>Sampling Level Comparison</h3>
		
		
		<div style="display: grid; grid-template-columns: 1fr 1fr 1fr; grid-gap: 15px; margin-bottom: 20px;">
			<figure style="margin: 0;">
				<img src="images/part4/bench_s1.png" alt="1 Sample Per Pixel" style="width:100%"/>
				<figcaption>1 Sample Per Pixel</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part4/bench_s2.png" alt="2 Samples Per Pixel" style="width:100%"/>
				<figcaption>2 Samples Per Pixel</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part4/bench_s4.png" alt="4 Samples Per Pixel" style="width:100%"/>
				<figcaption>4 Samples Per Pixel</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part4/bench_s8.png" alt="8 Samples Per Pixel" style="width:100%"/>
				<figcaption>8 Samples Per Pixel</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part4/bench_s16.png" alt="16 Samples Per Pixel" style="width:100%"/>
				<figcaption>16 Samples Per Pixel</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part4/bench_s64.png" alt="64 Samples Per Pixel" style="width:100%"/>
				<figcaption>64 Samples Per Pixel</figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part4/bench_s1024.png" alt="1024 Samples Per Pixel" style="width:100%"/>
				<figcaption>1024 Samples Per Pixel</figcaption>
			</figure>
		</div>
		
		
		
		<p>
			As the number of samples increases, we can observe:
		</p>
		<ul>
			<li>At 1 sample per pixel, the image shows significant noise and graininess</li>
			<li>By 8-16 samples, most of the harsh noise is reduced, but subtle graininess remains</li>
			<li>At 64 samples, the image appears quite smooth to the casual observer</li>
			<li>At 1024 samples, the image achieves near-perfect smoothness with minimal visible noise</li>
		</ul>
		
		<p>
			This demonstrates the classic trade-off in path tracing between image quality and computational cost. Each doubling of samples approximately doubles the rendering time, while the visual improvement follows a law of diminishing returns.
		</p>
		

		<h2>Part 5: Adaptive Sampling</h2>
		<p>
			Adaptive sampling is a technique that allocates more samples to complex regions of the image (high variance) and fewer samples to simpler regions (low variance). This approach can significantly reduce rendering time while maintaining visual quality.
		</p>
		
		<p>
			I implemented adaptive sampling by:
		</p>
		<ol>
			<li>Taking samples in batches (controlled by the samplesPerBatch parameter)</li>
			<li>After each batch, calculating the mean (μ) and variance (σ²) of the samples</li>
			<li>Computing a confidence interval I = 1.96 * σ/√n where n is the number of samples</li>
			<li>Stopping sampling when I ≤ maxTolerance * μ, indicating the pixel has converged</li>
		</ol>
		
		<p>
			Below are the results of adaptive sampling with sample per pixel 2048, samplesPerBatch=64, max ray depth 5, and maxTolerance=0.05:
		</p>
		
		<div style="display: grid; grid-template-columns: 1fr 1fr; grid-gap: 15px; margin-bottom: 20px;">
			<figure style="margin: 0;">
				<img src="images/part5/bunny.png" alt="Bunny Adaptive Sampling Render" style="width:100%"/>
				<figcaption>Bunny Adaptive Sampling Render </figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part5/bunny_rate.png" alt="Bunny Sample Distribution Heatmap" style="width:100%"/>
				<figcaption>Bunny Sample Distribution Heatmap </figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part5/dragon_as.png" alt="Adaptive Sampling Render" style="width:100%"/>
				<figcaption>Dragon Adaptive Sampling Render </figcaption>
			</figure>
			<figure style="margin: 0;">
				<img src="images/part5/dragon_as_rate.png" alt="Sample Distribution Heatmap" style="width:100%"/>
				<figcaption>Dragon Sample Distribution Heatmap </figcaption>
			</figure>
		</div>
		
		<p>
			The heatmap shows how adaptive sampling allocates resources efficiently:
		</p>
		<ul>
			<li>More samples (redder areas) are used for complex regions like shadow boundaries, reflections, and areas with fine detail</li>
			<li>Fewer samples (bluer areas) are used for simpler regions like flat surfaces with consistent lighting</li>
			<li>This approach achieves similar quality to uniform sampling while using fewer total samples</li>
		</ul>
		
		<h2>(Optional) Part 6: Extra Credit Opportunities</h2>
		<h3>BVH optimization</h3>		
		<p>The BVH implementation has been optimized in several ways:</p>
		<ol>
			<li> Surface Area Heuristic (SAH) - The BVH construction now uses a SAH-based approach instead of a simple median split which creates a more efficient BVH structure that minimizes ray traversal costs.**Surface Area Heuristic (SAH)** - The BVH construction now uses a SAH-based approach instead of a simple median split which creates a more efficient BVH structure that minimizes ray traversal costs.</li>
			<li> Stack-based Traversal - The recursive traversal has been replaced with a stack-based approach that eliminates function call overhead and is more cache-friendly.</li>
			<li> Spatial Coherence Cache - A caching mechanism that takes advantage of ray coherence, which is common in path tracing where nearby rays often follow similar paths.</li>
			<li> Front-to-Back Traversal - The traversal order has been optimized to visit nodes in front-to-back order based on the ray direction.</li>
		</ol>
		The SAH optimization saw an approximate 1.6-2x speedup. The combination of the stack-based traversal, spatial coherence cache, and front to back traversal saw a 3.5-4x speedup, seperate from the SAH optimization.

		To enable the profiling, please go to raytraced_renderer.cpp and turn on the relevant flags for the section:

		// Enable BVH profiling with all comparison methods
		BVHProfilingFlags flags;
		flags.enable_profiling = true;        // Turn on performance profiling
		flags.use_original_construction = false;  // Use SAH construction instead of median
		flags.use_recursive_traversal = true;  // Compare against recursive traversal
		flags.disable_spatial_cache = false;   // Enable spatial coherence cache
		flags.disable_front_to_back = false;   // Enable front-to-back traversal
		
		// Create BVH with 4 primitives per leaf and the profiling flags
		bvh = new BVHAccel(primitives, 4, flags);
		</div>
	</body>
</html>